---
layout: single
author_profile: true
title: Teaching
---

**Deep Learning for Natural Language Processing**<br/>
_Master AI, Fall 2019, Teaching Assistant_<br/>
This course teaches the essential architectures and parameter estimation procedures used in modern natural language processing. It spans NLP tasks from learning lexical representations, to part-of-speech tagging, to machine translation. Additionally, the students are introduced to Bayesian concepts for NLP, covering Bayesian neural networks and variational inference, Bayesian dropout, and variational auto-encoders for text.

**Basic Probability: Theory**<br/>
_Master of Logic, Fall 2019, Teaching Assistant_<br/>
This course teaches the basics of probability theory and statistics to students that have had no previous exposure to these topics. It covers probability theory concepts such as discrete and continuous random variables, conditional independence, Bayes' rule, correlation and covariance as well as fundamental statistics concepts such as maximum likelihood estimation and the EM algorithm.

**Natural Language Processing II**<br/>
_Master AI, Spring 2019, Teaching Assistant_<br/>
This course teaches about latent variable models for structured prediction for natural language processing. It covers models such as mixture models, HMMs, latent variable CRFs, and deep generative models; learning paradigms such as MLE and approximate posterior inference. [course website](https://uva-slpl.github.io/nlp2/)
